{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54279084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axd497/.usr/local/python/3.10.4/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import hiera\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage import exposure\n",
    "from skimage import segmentation\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.morphology import reconstruction\n",
    "from skimage.filters import sobel\n",
    "from skimage.transform import rescale\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "\n",
    "import sys\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c52292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/pan/SOM_CCCC_JGS25/durmaza/organoid_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d22a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hiera Model\n",
    "model = hiera.hiera_base_224(pretrained=True, checkpoint=\"mae_in1k\")\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dddd9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = glob.glob('3d gastruloids all images/MR/24H/*tif')\n",
    "mr24 = []\n",
    "for f in f_list:\n",
    "    temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "    im = transform_norm(temp/255.0)\n",
    "    output, intermediates = model(im[None,], return_intermediates=True)\n",
    "    mr24.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "mr24 = np.stack(mr24, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01743116",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = glob.glob('good_batch/MR/*tif')\n",
    "mr72 = []\n",
    "for f in f_list:\n",
    "    temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "    if temp.ndim < 3: continue\n",
    "    im = transform_norm(temp/255.0)\n",
    "    output, intermediates = model(im[None,], return_intermediates=True)\n",
    "    mr72.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "mr72 = np.stack(mr72, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96dfa4b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_list = glob.glob('3d gastruloids all images/GD/24H/*tif')\n",
    "gd24 = []\n",
    "for f in f_list:\n",
    "    temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "    if temp.ndim < 3: continue\n",
    "    im = transform_norm(temp/255.0)\n",
    "    output, intermediates = model(im[None,], return_intermediates=True)\n",
    "    gd24.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "gd24 = np.stack(gd24, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf61dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = glob.glob('good_batch/GD/*tif')\n",
    "gd72 = []\n",
    "for f in f_list:\n",
    "    temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "    if temp.ndim < 3: continue\n",
    "    im = transform_norm(temp/255.0)\n",
    "    output, intermediates = model(im[None,], return_intermediates=True)\n",
    "    gd72.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "gd72 = np.stack(gd72, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe577426",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = glob.glob('3d gastruloids all images/WT/24H/*tif')\n",
    "wt24 = []\n",
    "for f in f_list:\n",
    "    temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "    if temp.ndim < 3: continue\n",
    "    im = transform_norm(temp/255.0)\n",
    "    output, intermediates = model(im[None,], return_intermediates=True)\n",
    "    wt24.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "wt24 = np.stack(wt24, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8f37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = glob.glob('good_batch/WT/*tif')\n",
    "wt72 = []\n",
    "for f in f_list:\n",
    "    temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "    if temp.ndim < 3: continue\n",
    "    im = transform_norm(temp/255.0)\n",
    "    output, intermediates = model(im[None,], return_intermediates=True)\n",
    "    wt72.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "wt72 = np.stack(wt72, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1722a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "combined_im = np.vstack([mr24, mr72, gd24, gd72, wt24, wt72])\n",
    "combined_tag = np.concatenate([np.full(mr24.shape[0], fill_value=0),\n",
    "                               np.full(mr72.shape[0], fill_value=1),\n",
    "                               np.full(gd24.shape[0], fill_value=2),\n",
    "                               np.full(gd72.shape[0], fill_value=3),\n",
    "                               np.full(wt24.shape[0], fill_value=4),\n",
    "                               np.full(wt72.shape[0], fill_value=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b090e894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 7, 7, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0599fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 11:14:23.163073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-04 11:14:25.494363: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-04 11:14:55.680594: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-04 11:14:55.680639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (compt297): /proc/driver/nvidia/version does not exist\n",
      "2025-01-04 11:14:55.684545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## CNN Based\n",
    "import tensorflow as tf\n",
    "\n",
    "cnn_m = tf.keras.Sequential([tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                             tf.keras.layers.RandomRotation(0.7),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.GaussianNoise(0.1),\n",
    "                             tf.keras.layers.SpatialDropout2D(0.5),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), activation='gelu'),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), activation='gelu'),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(1e-3))])\n",
    "\n",
    "cnn_m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "515ff1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "10/10 [==============================] - 4s 45ms/step - loss: 3.1677 - val_loss: 1.9252\n",
      "Epoch 2/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.7748 - val_loss: 1.8436\n",
      "Epoch 3/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.6191 - val_loss: 1.7692\n",
      "Epoch 4/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.4415 - val_loss: 1.7025\n",
      "Epoch 5/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.1903 - val_loss: 1.6470\n",
      "Epoch 6/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.1211 - val_loss: 1.6047\n",
      "Epoch 7/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.1842 - val_loss: 1.5586\n",
      "Epoch 8/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.9806 - val_loss: 1.5169\n",
      "Epoch 9/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.0145 - val_loss: 1.4761\n",
      "Epoch 10/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 2.0383 - val_loss: 1.4487\n",
      "Epoch 11/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.8576 - val_loss: 1.4233\n",
      "Epoch 12/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.8304 - val_loss: 1.3883\n",
      "Epoch 13/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.8689 - val_loss: 1.3528\n",
      "Epoch 14/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.5850 - val_loss: 1.3195\n",
      "Epoch 15/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.8635 - val_loss: 1.2987\n",
      "Epoch 16/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.7059 - val_loss: 1.2720\n",
      "Epoch 17/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.7323 - val_loss: 1.2419\n",
      "Epoch 18/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.6879 - val_loss: 1.2187\n",
      "Epoch 19/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.5242 - val_loss: 1.1927\n",
      "Epoch 20/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.5761 - val_loss: 1.1819\n",
      "Epoch 21/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.5490 - val_loss: 1.1632\n",
      "Epoch 22/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 1.4284 - val_loss: 1.1485\n",
      "Epoch 23/150\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 1.5691 - val_loss: 1.1339\n",
      "Epoch 24/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.4130 - val_loss: 1.1223\n",
      "Epoch 25/150\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 1.5298 - val_loss: 1.1152\n",
      "Epoch 26/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.4121 - val_loss: 1.1172\n",
      "Epoch 27/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.4960 - val_loss: 1.1197\n",
      "Epoch 28/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.3968 - val_loss: 1.1231\n",
      "Epoch 29/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.3866 - val_loss: 1.1175\n",
      "Epoch 30/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.3256 - val_loss: 1.1021\n",
      "Epoch 31/150\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 1.3640 - val_loss: 1.0904\n",
      "Epoch 32/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.3199 - val_loss: 1.0837\n",
      "Epoch 33/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.3467 - val_loss: 1.0766\n",
      "Epoch 34/150\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 1.2456 - val_loss: 1.0755\n",
      "Epoch 35/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1881 - val_loss: 1.0684\n",
      "Epoch 36/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1799 - val_loss: 1.0553\n",
      "Epoch 37/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1585 - val_loss: 1.0406\n",
      "Epoch 38/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.2403 - val_loss: 1.0448\n",
      "Epoch 39/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.2257 - val_loss: 1.0303\n",
      "Epoch 40/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1838 - val_loss: 1.0328\n",
      "Epoch 41/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1465 - val_loss: 1.0222\n",
      "Epoch 42/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0809 - val_loss: 1.0105\n",
      "Epoch 43/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1778 - val_loss: 1.0119\n",
      "Epoch 44/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1610 - val_loss: 1.0182\n",
      "Epoch 45/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1095 - val_loss: 1.0166\n",
      "Epoch 46/150\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 1.0282 - val_loss: 1.0111\n",
      "Epoch 47/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0805 - val_loss: 1.0044\n",
      "Epoch 48/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 1.2133 - val_loss: 1.0058\n",
      "Epoch 49/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1808 - val_loss: 1.0021\n",
      "Epoch 50/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.1905 - val_loss: 1.0090\n",
      "Epoch 51/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9854 - val_loss: 1.0218\n",
      "Epoch 52/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0980 - val_loss: 1.0277\n",
      "Epoch 53/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0039 - val_loss: 1.0200\n",
      "Epoch 54/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9798 - val_loss: 1.0138\n",
      "Epoch 55/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0098 - val_loss: 0.9924\n",
      "Epoch 56/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9602 - val_loss: 0.9956\n",
      "Epoch 57/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0438 - val_loss: 0.9890\n",
      "Epoch 58/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0289 - val_loss: 0.9816\n",
      "Epoch 59/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9400 - val_loss: 0.9860\n",
      "Epoch 60/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9375 - val_loss: 0.9852\n",
      "Epoch 61/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9368 - val_loss: 0.9987\n",
      "Epoch 62/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8336 - val_loss: 1.0119\n",
      "Epoch 63/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0467 - val_loss: 1.0142\n",
      "Epoch 64/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9156 - val_loss: 0.9919\n",
      "Epoch 65/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9115 - val_loss: 0.9803\n",
      "Epoch 66/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9050 - val_loss: 0.9786\n",
      "Epoch 67/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8539 - val_loss: 0.9744\n",
      "Epoch 68/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8092 - val_loss: 0.9625\n",
      "Epoch 69/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8430 - val_loss: 0.9499\n",
      "Epoch 70/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8855 - val_loss: 0.9517\n",
      "Epoch 71/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8402 - val_loss: 0.9591\n",
      "Epoch 72/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9286 - val_loss: 0.9526\n",
      "Epoch 73/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8983 - val_loss: 0.9550\n",
      "Epoch 74/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8101 - val_loss: 0.9591\n",
      "Epoch 75/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8617 - val_loss: 0.9619\n",
      "Epoch 76/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9001 - val_loss: 0.9572\n",
      "Epoch 77/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8996 - val_loss: 0.9489\n",
      "Epoch 78/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9284 - val_loss: 0.9635\n",
      "Epoch 79/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8492 - val_loss: 0.9498\n",
      "Epoch 80/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8332 - val_loss: 0.9400\n",
      "Epoch 81/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7116 - val_loss: 0.9394\n",
      "Epoch 82/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7749 - val_loss: 0.9390\n",
      "Epoch 83/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8056 - val_loss: 0.9356\n",
      "Epoch 84/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7645 - val_loss: 0.9451\n",
      "Epoch 85/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7364 - val_loss: 0.9442\n",
      "Epoch 86/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8147 - val_loss: 0.9404\n",
      "Epoch 87/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7457 - val_loss: 0.9214\n",
      "Epoch 88/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7228 - val_loss: 0.9178\n",
      "Epoch 89/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7826 - val_loss: 0.9341\n",
      "Epoch 90/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8501 - val_loss: 0.9465\n",
      "Epoch 91/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7564 - val_loss: 0.9623\n",
      "Epoch 92/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7484 - val_loss: 0.9974\n",
      "Epoch 93/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7242 - val_loss: 1.0003\n",
      "Epoch 94/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7713 - val_loss: 0.9683\n",
      "Epoch 95/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7568 - val_loss: 0.9435\n",
      "Epoch 96/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7868 - val_loss: 0.9351\n",
      "Epoch 97/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6980 - val_loss: 0.9019\n",
      "Epoch 98/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6865 - val_loss: 0.8893\n",
      "Epoch 99/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7085 - val_loss: 0.8929\n",
      "Epoch 100/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7242 - val_loss: 0.8890\n",
      "Epoch 101/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.6999 - val_loss: 0.8875\n",
      "Epoch 102/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6328 - val_loss: 0.8700\n",
      "Epoch 103/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.6912 - val_loss: 0.8512\n",
      "Epoch 104/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7321 - val_loss: 0.8499\n",
      "Epoch 105/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6335 - val_loss: 0.8650\n",
      "Epoch 106/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6629 - val_loss: 0.8777\n",
      "Epoch 107/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6545 - val_loss: 0.8954\n",
      "Epoch 108/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7394 - val_loss: 0.9064\n",
      "Epoch 109/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6367 - val_loss: 0.8956\n",
      "Epoch 110/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6553 - val_loss: 0.8995\n",
      "Epoch 111/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6120 - val_loss: 0.9107\n",
      "Epoch 112/150\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.6110 - val_loss: 0.9316\n",
      "Epoch 113/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5623 - val_loss: 0.9301\n",
      "Epoch 114/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6139 - val_loss: 0.9278\n",
      "Epoch 115/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6414 - val_loss: 0.9302\n",
      "Epoch 116/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6029 - val_loss: 0.9435\n",
      "Epoch 117/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5483 - val_loss: 0.9432\n",
      "Epoch 118/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5714 - val_loss: 0.9303\n",
      "Epoch 119/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6203 - val_loss: 0.9354\n",
      "Epoch 120/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5224 - val_loss: 0.9247\n",
      "Epoch 121/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.6029 - val_loss: 0.9132\n",
      "Epoch 122/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5712 - val_loss: 0.8960\n",
      "Epoch 123/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5246 - val_loss: 0.8693\n",
      "Epoch 124/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5462 - val_loss: 0.8826\n",
      "Epoch 125/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5416 - val_loss: 0.9126\n",
      "Epoch 126/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5645 - val_loss: 0.9202\n",
      "Epoch 127/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5627 - val_loss: 0.9251\n",
      "Epoch 128/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5289 - val_loss: 0.9176\n",
      "Epoch 129/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5251 - val_loss: 0.9393\n",
      "Epoch 130/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5438 - val_loss: 0.9367\n",
      "Epoch 131/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5698 - val_loss: 0.9366\n",
      "Epoch 132/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5648 - val_loss: 0.9152\n",
      "Epoch 133/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5508 - val_loss: 0.9067\n",
      "Epoch 134/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4827 - val_loss: 0.9013\n",
      "Epoch 135/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5268 - val_loss: 0.8983\n",
      "Epoch 136/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5663 - val_loss: 0.8874\n",
      "Epoch 137/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5635 - val_loss: 0.8809\n",
      "Epoch 138/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5215 - val_loss: 0.8924\n",
      "Epoch 139/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5022 - val_loss: 0.9095\n",
      "Epoch 140/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4629 - val_loss: 0.8689\n",
      "Epoch 141/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4573 - val_loss: 0.8459\n",
      "Epoch 142/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5198 - val_loss: 0.8300\n",
      "Epoch 143/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4623 - val_loss: 0.8197\n",
      "Epoch 144/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5442 - val_loss: 0.8324\n",
      "Epoch 145/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5922 - val_loss: 0.8523\n",
      "Epoch 146/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4596 - val_loss: 0.8633\n",
      "Epoch 147/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4161 - val_loss: 0.8690\n",
      "Epoch 148/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5148 - val_loss: 0.8766\n",
      "Epoch 149/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4897 - val_loss: 0.9076\n",
      "Epoch 150/150\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.5308 - val_loss: 0.9168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f013c5b1090>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = np.eye(6)[combined_tag,]\n",
    "x_train, x_test, y_train, y_test = train_test_split(combined_im, one_hot, test_size=0.3, shuffle=True)\n",
    "cnn_m.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c0c5c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "emb_lay = cnn_m.layers[9](cnn_m.layers[8](cnn_m.layers[7](cnn_m.layers[6](cnn_m.layers[5](cnn_m.layers[4](cnn_m.layers[9](cnn_m.layers[3](cnn_m.layers[2](cnn_m.layers[1](cnn_m.layers[0](combined_im)))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1493a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Isomap\n",
    "pca_res = Isomap(n_neighbors=100).fit_transform(emb_lay)\n",
    "\n",
    "col_key = ['red', 'firebrick', 'dodgerblue', 'navy', 'lime', 'forestgreen']\n",
    "cust_leg = [Line2D([0], [0], color=s, lw=4) for s in col_key]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(pca_res[::,0], pca_res[::,1], c=[col_key[i] for i in combined_tag], alpha=0.6)\n",
    "ax.set_xlabel('Isomap 1')\n",
    "ax.set_ylabel('Isomap 2')\n",
    "ax.legend(cust_leg, ['MR 24', 'MR 72', 'GD 24', 'GD 72', 'WT 24', 'WT 72'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('isomap_hiera.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c831152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "tags = ['MR 24', 'MR 72', 'GD 24', 'GD 72', 'WT 24', 'WT 72']\n",
    "\n",
    "pred_res = cnn_m.predict(x_test)\n",
    "pred_res.shape\n",
    "res_cat = [tags[i] for i in np.argmax(pred_res, axis=-1)]\n",
    "res_test = [tags[i] for i in np.argmax(y_test, axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6568e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cnf = confusion_matrix(res_test, res_cat, labels=tags)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cnf, annot=True, linecolor='white', linewidths=1, ax=ax, xticklabels=tags, yticklabels=tags)\n",
    "\n",
    "plt.savefig('ConfusionMatrixCNN_K6_F30.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e79bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=3, n_estimators=1500,\n",
       "                       n_jobs=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=3, n_estimators=1500,\n",
       "                       n_jobs=4)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=3, n_estimators=1500,\n",
       "                       n_jobs=4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tags = ['MR 24', 'MR 72', 'GD 24', 'GD 72', 'WT 24', 'WT 72']\n",
    "\n",
    "X=combined_im.reshape([434, 7*7*768])\n",
    "y=[tags[i] for i in combined_tag]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf = RandomForestClassifier(max_depth=3, n_jobs=4, class_weight='balanced', n_estimators=1500)\n",
    "\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e917e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cnf = confusion_matrix(clf.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c936aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cnf, annot=True, linecolor='white', linewidths=1, ax=ax, xticklabels=tags, yticklabels=tags)\n",
    "\n",
    "plt.savefig('ConfusionMatrixRF.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65fe4908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6091954022988506"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1855e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN Based\n",
    "import tensorflow as tf\n",
    "\n",
    "cnn_m = tf.keras.Sequential([tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.SpatialDropout2D(0.5),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), activation='gelu'),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), activation='gelu'),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(1e-3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2be1783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d9fe42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot = LabelBinarizer().fit_transform(y)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58ffe39e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(combined_im, one_hot, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d950273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10/10 [==============================] - 10s 32ms/step - loss: 3.1148 - val_loss: 1.8846\n",
      "Epoch 2/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 2.8698 - val_loss: 1.8238\n",
      "Epoch 3/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 2.5273 - val_loss: 1.7709\n",
      "Epoch 4/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 2.3967 - val_loss: 1.7183\n",
      "Epoch 5/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 2.1999 - val_loss: 1.6657\n",
      "Epoch 6/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.9912 - val_loss: 1.6199\n",
      "Epoch 7/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.8921 - val_loss: 1.5760\n",
      "Epoch 8/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.9957 - val_loss: 1.5382\n",
      "Epoch 9/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.8748 - val_loss: 1.4924\n",
      "Epoch 10/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.4637 - val_loss: 1.4600\n",
      "Epoch 11/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.6704 - val_loss: 1.4351\n",
      "Epoch 12/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.6172 - val_loss: 1.4124\n",
      "Epoch 13/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.4956 - val_loss: 1.3879\n",
      "Epoch 14/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.2481 - val_loss: 1.3534\n",
      "Epoch 15/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.2768 - val_loss: 1.3285\n",
      "Epoch 16/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.2749 - val_loss: 1.3252\n",
      "Epoch 17/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.4126 - val_loss: 1.3323\n",
      "Epoch 18/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.3990 - val_loss: 1.3200\n",
      "Epoch 19/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.3015 - val_loss: 1.3104\n",
      "Epoch 20/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0886 - val_loss: 1.3080\n",
      "Epoch 21/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.1216 - val_loss: 1.3052\n",
      "Epoch 22/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0970 - val_loss: 1.2948\n",
      "Epoch 23/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.1826 - val_loss: 1.2761\n",
      "Epoch 24/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.1461 - val_loss: 1.2626\n",
      "Epoch 25/150\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.9302 - val_loss: 1.2479\n",
      "Epoch 26/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0946 - val_loss: 1.2587\n",
      "Epoch 27/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0704 - val_loss: 1.2519\n",
      "Epoch 28/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.9614 - val_loss: 1.2597\n",
      "Epoch 29/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.9700 - val_loss: 1.2631\n",
      "Epoch 30/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8252 - val_loss: 1.2707\n",
      "Epoch 31/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8785 - val_loss: 1.2912\n",
      "Epoch 32/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8125 - val_loss: 1.3071\n",
      "Epoch 33/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8510 - val_loss: 1.3018\n",
      "Epoch 34/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7907 - val_loss: 1.2718\n",
      "Epoch 35/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8337 - val_loss: 1.2488\n",
      "Epoch 36/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7970 - val_loss: 1.2487\n",
      "Epoch 37/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7758 - val_loss: 1.2587\n",
      "Epoch 38/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7629 - val_loss: 1.2764\n",
      "Epoch 39/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7612 - val_loss: 1.2858\n",
      "Epoch 40/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7285 - val_loss: 1.2715\n",
      "Epoch 41/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7886 - val_loss: 1.2639\n",
      "Epoch 42/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7339 - val_loss: 1.2725\n",
      "Epoch 43/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6883 - val_loss: 1.3022\n",
      "Epoch 44/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7298 - val_loss: 1.3069\n",
      "Epoch 45/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6807 - val_loss: 1.3149\n",
      "Epoch 46/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7468 - val_loss: 1.3111\n",
      "Epoch 47/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6824 - val_loss: 1.3318\n",
      "Epoch 48/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5888 - val_loss: 1.3276\n",
      "Epoch 49/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6009 - val_loss: 1.3142\n",
      "Epoch 50/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.6761 - val_loss: 1.2928\n",
      "Epoch 51/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5812 - val_loss: 1.2811\n",
      "Epoch 52/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5361 - val_loss: 1.2754\n",
      "Epoch 53/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5418 - val_loss: 1.2673\n",
      "Epoch 54/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5254 - val_loss: 1.2775\n",
      "Epoch 55/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5684 - val_loss: 1.2752\n",
      "Epoch 56/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5264 - val_loss: 1.2748\n",
      "Epoch 57/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5369 - val_loss: 1.2751\n",
      "Epoch 58/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4963 - val_loss: 1.2735\n",
      "Epoch 59/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5380 - val_loss: 1.2631\n",
      "Epoch 60/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5659 - val_loss: 1.2772\n",
      "Epoch 61/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4779 - val_loss: 1.2876\n",
      "Epoch 62/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4869 - val_loss: 1.2891\n",
      "Epoch 63/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5324 - val_loss: 1.2891\n",
      "Epoch 64/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4832 - val_loss: 1.2804\n",
      "Epoch 65/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.5206 - val_loss: 1.2877\n",
      "Epoch 66/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4435 - val_loss: 1.2858\n",
      "Epoch 67/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4434 - val_loss: 1.2896\n",
      "Epoch 68/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4962 - val_loss: 1.2865\n",
      "Epoch 69/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4401 - val_loss: 1.2787\n",
      "Epoch 70/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4295 - val_loss: 1.2644\n",
      "Epoch 71/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4740 - val_loss: 1.2580\n",
      "Epoch 72/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4228 - val_loss: 1.2708\n",
      "Epoch 73/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3990 - val_loss: 1.2800\n",
      "Epoch 74/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3802 - val_loss: 1.2916\n",
      "Epoch 75/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3889 - val_loss: 1.2887\n",
      "Epoch 76/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.4226 - val_loss: 1.3144\n",
      "Epoch 77/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3654 - val_loss: 1.3134\n",
      "Epoch 78/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.4219 - val_loss: 1.2994\n",
      "Epoch 79/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.4257 - val_loss: 1.3146\n",
      "Epoch 80/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3691 - val_loss: 1.3286\n",
      "Epoch 81/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3391 - val_loss: 1.3299\n",
      "Epoch 82/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3851 - val_loss: 1.3312\n",
      "Epoch 83/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.4533 - val_loss: 1.3341\n",
      "Epoch 84/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3678 - val_loss: 1.3093\n",
      "Epoch 85/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3489 - val_loss: 1.2888\n",
      "Epoch 86/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3396 - val_loss: 1.2684\n",
      "Epoch 87/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3454 - val_loss: 1.2398\n",
      "Epoch 88/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3744 - val_loss: 1.2312\n",
      "Epoch 89/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3996 - val_loss: 1.2203\n",
      "Epoch 90/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3427 - val_loss: 1.2194\n",
      "Epoch 91/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.3246 - val_loss: 1.2126\n",
      "Epoch 92/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3444 - val_loss: 1.2233\n",
      "Epoch 93/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3146 - val_loss: 1.2561\n",
      "Epoch 94/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3301 - val_loss: 1.2708\n",
      "Epoch 95/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3726 - val_loss: 1.2768\n",
      "Epoch 96/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3606 - val_loss: 1.2671\n",
      "Epoch 97/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3290 - val_loss: 1.2556\n",
      "Epoch 98/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2792 - val_loss: 1.2492\n",
      "Epoch 99/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2861 - val_loss: 1.2415\n",
      "Epoch 100/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3041 - val_loss: 1.2312\n",
      "Epoch 101/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3036 - val_loss: 1.2351\n",
      "Epoch 102/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.3353 - val_loss: 1.2640\n",
      "Epoch 103/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.3148 - val_loss: 1.2465\n",
      "Epoch 104/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3056 - val_loss: 1.2525\n",
      "Epoch 105/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2818 - val_loss: 1.2566\n",
      "Epoch 106/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3749 - val_loss: 1.2556\n",
      "Epoch 107/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2764 - val_loss: 1.2719\n",
      "Epoch 108/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2952 - val_loss: 1.2497\n",
      "Epoch 109/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.3087 - val_loss: 1.2285\n",
      "Epoch 110/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2731 - val_loss: 1.2199\n",
      "Epoch 111/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2701 - val_loss: 1.2199\n",
      "Epoch 112/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2835 - val_loss: 1.2354\n",
      "Epoch 113/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2624 - val_loss: 1.2440\n",
      "Epoch 114/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2842 - val_loss: 1.2549\n",
      "Epoch 115/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.3160 - val_loss: 1.2672\n",
      "Epoch 116/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2789 - val_loss: 1.2735\n",
      "Epoch 117/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2825 - val_loss: 1.2625\n",
      "Epoch 118/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2237 - val_loss: 1.2677\n",
      "Epoch 119/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2281 - val_loss: 1.2764\n",
      "Epoch 120/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2939 - val_loss: 1.2789\n",
      "Epoch 121/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2897 - val_loss: 1.2868\n",
      "Epoch 122/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2791 - val_loss: 1.2973\n",
      "Epoch 123/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2300 - val_loss: 1.3015\n",
      "Epoch 124/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2683 - val_loss: 1.3084\n",
      "Epoch 125/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2499 - val_loss: 1.3002\n",
      "Epoch 126/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2105 - val_loss: 1.3041\n",
      "Epoch 127/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2316 - val_loss: 1.3237\n",
      "Epoch 128/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2338 - val_loss: 1.3339\n",
      "Epoch 129/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2556 - val_loss: 1.3330\n",
      "Epoch 130/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2297 - val_loss: 1.3345\n",
      "Epoch 131/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2240 - val_loss: 1.3284\n",
      "Epoch 132/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2588 - val_loss: 1.3378\n",
      "Epoch 133/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2425 - val_loss: 1.3278\n",
      "Epoch 134/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2328 - val_loss: 1.3297\n",
      "Epoch 135/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2226 - val_loss: 1.3367\n",
      "Epoch 136/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2198 - val_loss: 1.3486\n",
      "Epoch 137/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2436 - val_loss: 1.3628\n",
      "Epoch 138/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2057 - val_loss: 1.3783\n",
      "Epoch 139/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2291 - val_loss: 1.3691\n",
      "Epoch 140/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2456 - val_loss: 1.3641\n",
      "Epoch 141/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2248 - val_loss: 1.3596\n",
      "Epoch 142/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.1970 - val_loss: 1.3613\n",
      "Epoch 143/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2039 - val_loss: 1.3787\n",
      "Epoch 144/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2787 - val_loss: 1.3898\n",
      "Epoch 145/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2150 - val_loss: 1.3693\n",
      "Epoch 146/150\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2333 - val_loss: 1.3602\n",
      "Epoch 147/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.1965 - val_loss: 1.3432\n",
      "Epoch 148/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2053 - val_loss: 1.3401\n",
      "Epoch 149/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2345 - val_loss: 1.3354\n",
      "Epoch 150/150\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2413 - val_loss: 1.3505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff21c5eb6a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_m.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79168218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_res = cnn_m.predict(x_test)\n",
    "pred_res.shape\n",
    "res_cat = [tags[i] for i in np.argmax(pred_res, axis=-1)]\n",
    "res_test = [tags[i] for i in np.argmax(y_test, axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e91f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cnf = confusion_matrix(res_test, res_cat)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cnf, annot=True, linecolor='white', linewidths=1, ax=ax, xticklabels=tags, yticklabels=tags)\n",
    "\n",
    "plt.savefig('ConfusionMatrixCNN.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f724a229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff21c567af0>,\n",
       " <keras.layers.regularization.spatial_dropout2d.SpatialDropout2D at 0x7ff21c567430>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff21c75fb80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7ff21c75f8e0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x7ff21c60faf0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff21c6f5d80>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x7ff21c6f69b0>,\n",
       " <keras.layers.core.dense.Dense at 0x7ff21c538250>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_m.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "282f3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lay = cnn_m.layers[5](cnn_m.layers[4](cnn_m.layers[3](cnn_m.layers[2](cnn_m.layers[1](cnn_m.layers[0](combined_im, training=False), training=False)))), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b9351b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_res = Isomap(n_neighbors=30).fit_transform(emb_lay)\n",
    "\n",
    "col_key = ['red', 'sandybrown', 'cyan', 'navy', 'darkviolet', 'forestgreen']\n",
    "cust_leg = [Line2D([0], [0], color=s, lw=4) for s in col_key]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(pca_res[::,0], pca_res[::,1], c=[col_key[i] for i in combined_tag], alpha=0.6)\n",
    "ax.set_xlabel('Isomap 1')\n",
    "ax.set_ylabel('Isomap 2')\n",
    "ax.legend(cust_leg, ['MR 24', 'MR 72', 'GD 24', 'GD 72', 'WT 24', 'WT 72'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('isomap_hiera_cnn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9c642d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6641221374045801"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.diag(cnf))/np.sum(cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de81fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_im(path=\"\"):\n",
    "    f_list = glob.glob(path)\n",
    "    local_data = []\n",
    "    for f in f_list:\n",
    "        temp=io.imread(f, as_gray=False, plugin='tifffile').astype(np.float32)\n",
    "        im = transform_norm(temp/255.0)\n",
    "        output, intermediates = model(im[None,], return_intermediates=True)\n",
    "        local_data.append(intermediates[3][0,].detach().numpy())\n",
    "\n",
    "    return(np.stack(local_data, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16f4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AKT Inhibition\n",
    "gd72_neg = np.vstack([load_im('gd neg/72*tif'), load_im('gd neg/gd neg*tif')])\n",
    "gd72_pos = np.vstack([load_im('gd pos/72*tif'), load_im('gd pos/gd pos*tif')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9908866",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr72_neg = np.vstack([load_im('mr neg/72*tif'), load_im('mr neg/mr neg*tif')])\n",
    "mr72_pos = np.vstack([load_im('mr pos/72*tif'), load_im('mr pos/mr pos*tif')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc089f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt72_neg = np.vstack([load_im('wt neg/72*tif'), load_im('wt neg/wt neg*tif')])\n",
    "wt72_pos = np.vstack([load_im('wt pos/72*tif'), load_im('wt pos/wt pos*tif')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c10900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 10:37:03.913028: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-26 10:37:07.827713: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-26 10:38:10.277961: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-01-26 10:38:10.278009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (compt379): /proc/driver/nvidia/version does not exist\n",
      "2025-01-26 10:38:10.290655: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## CNN Based\n",
    "import tensorflow as tf\n",
    "\n",
    "cnn_m = tf.keras.Sequential([tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                             tf.keras.layers.RandomRotation(0.7),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.GaussianNoise(0.1),\n",
    "                             tf.keras.layers.SpatialDropout2D(0.5),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), activation='gelu'),\n",
    "                             tf.keras.layers.Conv2D(32, (3, 3), activation='gelu'),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Dropout(0.5),\n",
    "                             tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(1e-3))])\n",
    "\n",
    "cnn_m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14d4e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "combined_im = np.vstack([mr72_neg, mr72_pos, gd72_neg, gd72_pos, wt72_neg, wt72_pos])\n",
    "combined_tag = np.concatenate([np.full(mr72_neg.shape[0], fill_value=0),\n",
    "                               np.full(mr72_pos.shape[0], fill_value=1),\n",
    "                               np.full(gd72_neg.shape[0], fill_value=2),\n",
    "                               np.full(gd72_pos.shape[0], fill_value=3),\n",
    "                               np.full(wt72_neg.shape[0], fill_value=4), \n",
    "                               np.full(wt72_pos.shape[0], fill_value=5)])\n",
    "\n",
    "one_hot = np.eye(6)[combined_tag,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d215ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tags = ['MR 72_Neg', 'MR 72_Pos',\n",
    "       'GD 72_Neg', 'GD 72_Pos',\n",
    "       'WT 72_Neg', 'WT 72_Pos']\n",
    "\n",
    "#tags = ['Intermediate', 'Low', 'High']\n",
    "\n",
    "y=[tags[i] for i in combined_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e248eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(combined_im, one_hot, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ec2e7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "21/21 [==============================] - 4s 23ms/step - loss: 3.1558 - val_loss: 2.0395\n",
      "Epoch 2/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 3.1009 - val_loss: 1.9750\n",
      "Epoch 3/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.6929 - val_loss: 1.9437\n",
      "Epoch 4/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.8034 - val_loss: 1.9173\n",
      "Epoch 5/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.5902 - val_loss: 1.9078\n",
      "Epoch 6/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.6807 - val_loss: 1.8509\n",
      "Epoch 7/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.6239 - val_loss: 1.8014\n",
      "Epoch 8/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.6364 - val_loss: 1.7511\n",
      "Epoch 9/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.5392 - val_loss: 1.7705\n",
      "Epoch 10/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.5171 - val_loss: 1.7965\n",
      "Epoch 11/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.3695 - val_loss: 1.7821\n",
      "Epoch 12/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 2.2429 - val_loss: 1.8275\n",
      "Epoch 13/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.1180 - val_loss: 1.8148\n",
      "Epoch 14/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.3668 - val_loss: 1.7140\n",
      "Epoch 15/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 2.0529 - val_loss: 1.6853\n",
      "Epoch 16/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 2.2917 - val_loss: 1.6489\n",
      "Epoch 17/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.9445 - val_loss: 1.6390\n",
      "Epoch 18/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.8751 - val_loss: 1.5936\n",
      "Epoch 19/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.9686 - val_loss: 1.5794\n",
      "Epoch 20/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.8005 - val_loss: 1.5600\n",
      "Epoch 21/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.9092 - val_loss: 1.5717\n",
      "Epoch 22/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.7712 - val_loss: 1.5677\n",
      "Epoch 23/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.7776 - val_loss: 1.5671\n",
      "Epoch 24/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.8853 - val_loss: 1.5666\n",
      "Epoch 25/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.6868 - val_loss: 1.5278\n",
      "Epoch 26/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.8793 - val_loss: 1.5044\n",
      "Epoch 27/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.6839 - val_loss: 1.5097\n",
      "Epoch 28/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.6941 - val_loss: 1.5067\n",
      "Epoch 29/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.6911 - val_loss: 1.4521\n",
      "Epoch 30/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.6354 - val_loss: 1.3877\n",
      "Epoch 31/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.5492 - val_loss: 1.3573\n",
      "Epoch 32/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.5398 - val_loss: 1.3172\n",
      "Epoch 33/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.5788 - val_loss: 1.3549\n",
      "Epoch 34/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.4309 - val_loss: 1.3895\n",
      "Epoch 35/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.4301 - val_loss: 1.3655\n",
      "Epoch 36/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.4645 - val_loss: 1.3321\n",
      "Epoch 37/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.3532 - val_loss: 1.3065\n",
      "Epoch 38/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.3126 - val_loss: 1.2720\n",
      "Epoch 39/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.4237 - val_loss: 1.2529\n",
      "Epoch 40/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.4882 - val_loss: 1.2192\n",
      "Epoch 41/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.3492 - val_loss: 1.1778\n",
      "Epoch 42/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2886 - val_loss: 1.1355\n",
      "Epoch 43/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2791 - val_loss: 1.1187\n",
      "Epoch 44/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2977 - val_loss: 1.1539\n",
      "Epoch 45/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2522 - val_loss: 1.1411\n",
      "Epoch 46/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2640 - val_loss: 1.1280\n",
      "Epoch 47/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.3392 - val_loss: 1.1259\n",
      "Epoch 48/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2701 - val_loss: 1.1356\n",
      "Epoch 49/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.2796 - val_loss: 1.1152\n",
      "Epoch 50/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.2717 - val_loss: 1.0922\n",
      "Epoch 51/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.0531 - val_loss: 1.0802\n",
      "Epoch 52/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.1911 - val_loss: 1.0560\n",
      "Epoch 53/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.1501 - val_loss: 1.0387\n",
      "Epoch 54/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.1666 - val_loss: 1.0304\n",
      "Epoch 55/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.1990 - val_loss: 1.0143\n",
      "Epoch 56/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.1797 - val_loss: 1.0281\n",
      "Epoch 57/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.1600 - val_loss: 1.0257\n",
      "Epoch 58/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.1651 - val_loss: 1.0216\n",
      "Epoch 59/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.0806 - val_loss: 1.0363\n",
      "Epoch 60/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.1379 - val_loss: 1.0585\n",
      "Epoch 61/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.0528 - val_loss: 1.0363\n",
      "Epoch 62/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9738 - val_loss: 1.0091\n",
      "Epoch 63/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.1302 - val_loss: 0.9972\n",
      "Epoch 64/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.0422 - val_loss: 0.9887\n",
      "Epoch 65/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.0688 - val_loss: 0.9810\n",
      "Epoch 66/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9927 - val_loss: 0.9423\n",
      "Epoch 67/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9961 - val_loss: 0.9095\n",
      "Epoch 68/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.9699 - val_loss: 0.9160\n",
      "Epoch 69/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.0792 - val_loss: 0.9269\n",
      "Epoch 70/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9848 - val_loss: 0.9294\n",
      "Epoch 71/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8815 - val_loss: 0.9155\n",
      "Epoch 72/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.9741 - val_loss: 0.9272\n",
      "Epoch 73/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9141 - val_loss: 0.9567\n",
      "Epoch 74/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.9276 - val_loss: 0.9938\n",
      "Epoch 75/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8950 - val_loss: 0.9313\n",
      "Epoch 76/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 1.0110 - val_loss: 0.9160\n",
      "Epoch 77/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 1.0010 - val_loss: 0.9194\n",
      "Epoch 78/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9356 - val_loss: 0.9279\n",
      "Epoch 79/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8573 - val_loss: 0.9359\n",
      "Epoch 80/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8595 - val_loss: 0.9637\n",
      "Epoch 81/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.9077 - val_loss: 0.9633\n",
      "Epoch 82/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9813 - val_loss: 0.9181\n",
      "Epoch 83/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9055 - val_loss: 0.9144\n",
      "Epoch 84/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9090 - val_loss: 0.9300\n",
      "Epoch 85/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8712 - val_loss: 0.9231\n",
      "Epoch 86/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7849 - val_loss: 0.9102\n",
      "Epoch 87/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.9115 - val_loss: 0.8827\n",
      "Epoch 88/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8180 - val_loss: 0.8732\n",
      "Epoch 89/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8099 - val_loss: 0.9057\n",
      "Epoch 90/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7835 - val_loss: 0.9049\n",
      "Epoch 91/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7887 - val_loss: 0.8987\n",
      "Epoch 92/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.8489 - val_loss: 0.8980\n",
      "Epoch 93/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7524 - val_loss: 0.9043\n",
      "Epoch 94/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8237 - val_loss: 0.8680\n",
      "Epoch 95/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7860 - val_loss: 0.8698\n",
      "Epoch 96/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7974 - val_loss: 0.9072\n",
      "Epoch 97/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7865 - val_loss: 0.9023\n",
      "Epoch 98/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.8360 - val_loss: 0.8955\n",
      "Epoch 99/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7142 - val_loss: 0.8979\n",
      "Epoch 100/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7212 - val_loss: 0.9054\n",
      "Epoch 101/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7270 - val_loss: 0.8896\n",
      "Epoch 102/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7083 - val_loss: 0.8553\n",
      "Epoch 103/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7652 - val_loss: 0.8468\n",
      "Epoch 104/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7115 - val_loss: 0.8580\n",
      "Epoch 105/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7229 - val_loss: 0.8536\n",
      "Epoch 106/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7488 - val_loss: 0.8421\n",
      "Epoch 107/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6727 - val_loss: 0.8189\n",
      "Epoch 108/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6757 - val_loss: 0.8384\n",
      "Epoch 109/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7451 - val_loss: 0.8382\n",
      "Epoch 110/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.7056 - val_loss: 0.8439\n",
      "Epoch 111/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6203 - val_loss: 0.8663\n",
      "Epoch 112/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6611 - val_loss: 0.8878\n",
      "Epoch 113/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6855 - val_loss: 0.8435\n",
      "Epoch 114/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.7233 - val_loss: 0.8085\n",
      "Epoch 115/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6231 - val_loss: 0.8559\n",
      "Epoch 116/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6399 - val_loss: 0.8773\n",
      "Epoch 117/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6454 - val_loss: 0.8817\n",
      "Epoch 118/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6848 - val_loss: 0.9265\n",
      "Epoch 119/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6667 - val_loss: 0.8907\n",
      "Epoch 120/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5740 - val_loss: 0.8740\n",
      "Epoch 121/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6739 - val_loss: 0.8574\n",
      "Epoch 122/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6353 - val_loss: 0.8502\n",
      "Epoch 123/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6360 - val_loss: 0.8553\n",
      "Epoch 124/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5995 - val_loss: 0.8619\n",
      "Epoch 125/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6438 - val_loss: 0.9183\n",
      "Epoch 126/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5643 - val_loss: 0.9003\n",
      "Epoch 127/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5803 - val_loss: 0.8939\n",
      "Epoch 128/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5246 - val_loss: 0.8692\n",
      "Epoch 129/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5451 - val_loss: 0.8878\n",
      "Epoch 130/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5805 - val_loss: 0.8997\n",
      "Epoch 131/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6681 - val_loss: 0.9460\n",
      "Epoch 132/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5632 - val_loss: 0.8619\n",
      "Epoch 133/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6672 - val_loss: 0.8302\n",
      "Epoch 134/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4982 - val_loss: 0.8566\n",
      "Epoch 135/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.4970 - val_loss: 0.8487\n",
      "Epoch 136/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5440 - val_loss: 0.8587\n",
      "Epoch 137/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5580 - val_loss: 0.8831\n",
      "Epoch 138/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5785 - val_loss: 0.9107\n",
      "Epoch 139/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5746 - val_loss: 0.9161\n",
      "Epoch 140/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.6214 - val_loss: 0.8869\n",
      "Epoch 141/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5768 - val_loss: 0.8737\n",
      "Epoch 142/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5701 - val_loss: 0.8908\n",
      "Epoch 143/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5983 - val_loss: 0.8685\n",
      "Epoch 144/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5515 - val_loss: 0.8891\n",
      "Epoch 145/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5893 - val_loss: 0.8548\n",
      "Epoch 146/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5803 - val_loss: 0.8366\n",
      "Epoch 147/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.5130 - val_loss: 0.8306\n",
      "Epoch 148/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.6063 - val_loss: 0.8387\n",
      "Epoch 149/150\n",
      "21/21 [==============================] - 0s 15ms/step - loss: 0.5280 - val_loss: 0.8258\n",
      "Epoch 150/150\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.4903 - val_loss: 0.8110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d1297ee30>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_m.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=150, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddeeb109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_res = cnn_m.predict(x_test)\n",
    "pred_res.shape\n",
    "res_cat = [tags[i] for i in np.argmax(pred_res, axis=-1)]\n",
    "res_test = [tags[i] for i in np.argmax(y_test, axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62183e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cnf = confusion_matrix(res_test, res_cat, labels=tags)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cnf, annot=True, linecolor='white', linewidths=1, ax=ax, xticklabels=tags, yticklabels=tags)\n",
    "\n",
    "plt.savefig('ConfusionMatrixCNN_Akt_K6_F30.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e278b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_m.save('cnn_model_akt_k6_f30.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0106f29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "cnn_m = tf.keras.models.load_model('cnn_model_akt.keras')\n",
    "emb_lay = cnn_m.layers[9](cnn_m.layers[8](cnn_m.layers[7](cnn_m.layers[6](cnn_m.layers[5](cnn_m.layers[4](cnn_m.layers[9](cnn_m.layers[3](cnn_m.layers[2](cnn_m.layers[1](cnn_m.layers[0](combined_im)))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8efb48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['MR 72_Neg', 'MR 72_Pos',\n",
    "       'GD 72_Neg', 'GD 72_Pos',\n",
    "       'WT 72_Neg', 'WT 72_Pos']\n",
    "\n",
    "#tags = ['Intermediate', 'Low', 'High']\n",
    "\n",
    "pca_res = Isomap(n_neighbors=100).fit_transform(emb_lay)\n",
    "\n",
    "col_key = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "cust_leg = [Line2D([0], [0], color=s, lw=4) for s in col_key]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(pca_res[::,0], pca_res[::,1], c=[col_key[i] for i in combined_tag], alpha=0.6)\n",
    "ax.set_xlabel('Isomap 1')\n",
    "ax.set_ylabel('Isomap 2')\n",
    "ax.legend(cust_leg, tags, loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('isomap_hiera_cnn_akt_model_k6.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac5466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
